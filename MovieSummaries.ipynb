{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob \n",
    "import csv\n",
    "import ast\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "'''read input csv file which has the information pertaining to the movie id, name and genre to which a movie belongs to'''\n",
    "def read_Movie_Id_Name_Genre_File(filePath):\n",
    "    movie_id_name_genre_dicts_list = []\n",
    "    count = 0\n",
    "    with open(filePath) as file:\n",
    "        \n",
    "        movieFile = csv.reader(file, delimiter = '\\t')\n",
    "        for row in movieFile:\n",
    "            \n",
    "            movie_Id = row[0]\n",
    "            movie_Name = row[2]\n",
    "            genreListOfTheMovie = getTheGenresFromString(row[-1])\n",
    "            movie_id_name_genre_dicts_list.append(get_the_genres_dict_of_a_movie(movie_Id, movie_Name, genreListOfTheMovie))\n",
    "            \n",
    "    return pd.DataFrame(movie_id_name_genre_dicts_list)\n",
    "\n",
    "\n",
    "####################\n",
    "\n",
    "def getTheGenresFromString(genreString):\n",
    "    '''This function receives a genre string (of dictionary form) and returns a list of genres to which a movie belongs to'''\n",
    "    genreOfTheMovie = []\n",
    "    \n",
    "    '''ast.literal_eval() converts a  string of type \"/m/09c7w0\": \"United States of America\" to a dictionary of \n",
    "    {/m/09c7w0\": \"United States of America}'''\n",
    "    genreDictionary = ast.literal_eval(genreString)\n",
    "    for _, genre in genreDictionary.items():\n",
    "        genreOfTheMovie.append(genre)\n",
    "    \n",
    "    return genreOfTheMovie  \n",
    "\n",
    "\n",
    "#####################\n",
    "\n",
    "def get_the_genres_dict_of_a_movie(movie_Id, movie_Name, movie_genres_list):\n",
    "    '''input data has genres of type Action/Comedy and also Romantic comedy'''\n",
    "    whitespace = \" \"\n",
    "    backslash = \"/\"\n",
    "    movie_id_name_genre_dict = {'movie_Id' : movie_Id, 'movie_Name' : movie_Name}\n",
    "    genres = []\n",
    "    \n",
    "    for every_genre in movie_genres_list:\n",
    "        if whitespace in every_genre:\n",
    "            genres = genres + every_genre.split(whitespace)\n",
    "        elif backslash in every_genre:\n",
    "            genres = genres + every_genre.split(backslash)\n",
    "        else:\n",
    "            genres.append(every_genre)\n",
    "        \n",
    "    genres_list = create_genres_dict(genres)\n",
    "        \n",
    "    return add_genre_tuples_to_the_dict(movie_id_name_genre_dict, genres_list)\n",
    "\n",
    "\n",
    "\n",
    "#####################\n",
    "\n",
    "def create_genres_dict(genres):\n",
    "    \n",
    "    primary_genres = ['Action', 'Adventure', 'Animation', 'Biopic', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Family', 'Fantasy', 'Film-noir',\n",
    "         'History', 'Horror', 'Music', 'Musical', 'Mystery', 'News', 'Prison', 'Reality', 'Romance', 'Science Fiction', 'Sport', 'Thriller', 'War', 'Western']\n",
    "    \n",
    "    genres_list = []\n",
    "    if len(genres) == 1 and genres[0] in primary_genres:\n",
    "        #movie_id_name_genre_dict[genres[0]] = 1\n",
    "        genres_list.append(genres[0])\n",
    "    else:\n",
    "        for genre in genres:\n",
    "            if genre in primary_genres:\n",
    "                #movie_id_name_genre_dict[every_genre] = 1\n",
    "                genres_list.append(genre)\n",
    "                \n",
    "    return genres_list\n",
    "\n",
    "\n",
    "######################\n",
    "    \n",
    "def add_genre_tuples_to_the_dict(movie_id_name_genre_dict, genres_list):\n",
    "    for genre in genres_list:\n",
    "        movie_id_name_genre_dict[genre] = 1 \n",
    "    \n",
    "    return movie_id_name_genre_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "def read_movie_plot_summary_file(filePath):\n",
    "    movie_id_plot_summary_dicts_list = []\n",
    "    \n",
    "    \n",
    "    with open(filePath) as file:\n",
    "        moviePlotSummaryFile = csv.reader(file, delimiter = '\\t')\n",
    "        \n",
    "        for row in moviePlotSummaryFile:\n",
    "            #print(\"before - \" + str(len(row[1])))\n",
    "            movie_id_plot_summary_dict = {}\n",
    "            lemmatizedSummary = list(set(lemmatizeAndRemoveStopWordsFromTheSummary(row[1])))\n",
    "            \n",
    "            \n",
    "            '''converting a list of string words to a string separated by a whitespace'''\n",
    "            lemmatizedSummary_words_string = \" \".join(lemmatizedSummary)\n",
    "            \n",
    "            movie_id_plot_summary_dict['movie_Id'] = row[0]\n",
    "            movie_id_plot_summary_dict['plot_summary_tokens'] = lemmatizedSummary_words_string\n",
    "            \n",
    "            movie_id_plot_summary_dicts_list.append(movie_id_plot_summary_dict)\n",
    "                                      \n",
    "    return pd.DataFrame(movie_id_plot_summary_dicts_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################\n",
    "\n",
    "def lemmatizeAndRemoveStopWordsFromTheSummary(plotSummary):\n",
    "    wordLemmatizer = WordNetLemmatizer()\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "#     punctuations = \"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\n",
    "    \n",
    "    summary_words = nltk.word_tokenize(plotSummary.lower())\n",
    "    modified_summary = []\n",
    "    #wordIndex = 0\n",
    "    \n",
    "    for word in summary_words:\n",
    "            \n",
    "        if not word in stopWords and len(word) > 2:\n",
    "            \n",
    "            lemmatizedWord = wordLemmatizer.lemmatize(remove_punctuation_marks(word), pos = \"v\")\n",
    "            modified_summary.append(lemmatizedWord)\n",
    "                \n",
    "    return modified_summary\n",
    "\n",
    "\n",
    "\n",
    "def remove_punctuation_marks(word):\n",
    "    #return re.sub('[!\\\"#$%&\\'\\'()*+,-./:;<=>?@\\[\\]^_{|}~`\\s+]', '', word)\n",
    "    return re.sub('[0-9]+$', '', (re.sub('[!\\\"#$%&\\'\\'()*+,-./:;<=>?@\\[\\]^_{|}~`\\s+]', '', word)))\n",
    "\n",
    "\n",
    "#######################\n",
    "\n",
    "def convert_text_data_to_matrix_of_tokens(text_data):\n",
    "    count_vectorizer = CountVectorizer(analyzer = 'word', tokenizer = None, preprocessor = None, stop_words = None, max_features = 3000)\n",
    "    transformed_data = count_vectorizer.fit_transform(text_data)\n",
    "    return pd.DataFrame(transformed_data.toarray())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#######################################################################################################################\n",
    "###################################################################\n",
    "\n",
    "def merge_the_movieIdNameGenre_plot_Dfs(movie_Id_Name_Genre_Df, movie_plot_Df):\n",
    "    return pd.merge(movie_Id_Name_Genre_Df, movie_plot_Df, on = 'movie_Id')\n",
    "\n",
    "\n",
    "####################################################################\n",
    "\n",
    "def fill_NaNs_with_Zeros(movie_id_name_genre_plot_df):\n",
    "    return movie_id_name_genre_plot_df.fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "###################################################################\n",
    "def get_transformed_train_test_data(movie_id_name_genre_plot_df):\n",
    "    \n",
    "    #test_columns = [column for column in movie_id_name_genre_plot_df.columns if column not in ['movie_Id', 'movie_Name', 'plot_summary_tokens']]\n",
    "    test_columns = [column for column in movie_id_name_genre_plot_df.columns if column not in ['plot_summary_tokens',\n",
    "                                                                                    'movie_Id', 'movie_Name']]    \n",
    "    train_data = movie_id_name_genre_plot_df.loc[:,'plot_summary_tokens']\n",
    "    test_data = movie_id_name_genre_plot_df.loc[:, test_columns]\n",
    "    \n",
    "    #train_data_transformed = convert_text_data_to_matrix_of_tokens(train_data)\n",
    "    \n",
    "    \n",
    "    return [train_data, test_data]\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "def get_train_and_label_data(train_data, test_data):\n",
    "    ## train_data is a list object\n",
    "    \n",
    "    #train_X = train_data.loc[0:35000,]\n",
    "    train_X = train_data[0 : 35000]\n",
    "    train_Y = test_data.loc[0:34999,]\n",
    "    \n",
    "    #test_X = train_data.loc[35000: ,]\n",
    "    test_X = train_data[35000 : ]\n",
    "    test_Y = test_data.loc[35000: ,]\n",
    "    \n",
    "    return [[train_X, train_Y], [test_X, test_Y]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_id_name_genre_df = read_Movie_Id_Name_Genre_File('/Users/vijay/Downloads/Datasets/MovieSummaries/movie.metadata.tsv')\n",
    "movie_id_plot_summary_df = read_movie_plot_summary_file('/Users/vijay/Downloads/Datasets/MovieSummaries/plot_summaries.txt')\n",
    "movie_id_name_genre_plot_df = merge_the_movieIdNameGenre_plot_Dfs(movie_id_name_genre_df, movie_id_plot_summary_df)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_id_name_genre_plot_df_zeros = fill_NaNs_with_Zeros(movie_id_name_genre_plot_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_test_data = get_transformed_train_test_data(movie_id_name_genre_plot_df_zeros)\n",
    "train_data = train_test_data[0]\n",
    "test_data = train_test_data[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "\n",
    "'''\n",
    "Out of all the words in plot_summary_tokens, only those which have occurred more than 15\n",
    "times in all the training samples were used in BoW representation. \n",
    "'''\n",
    "\n",
    "def get_summary_as_dictionary_of_words(summary):\n",
    "    return Counter(summary)\n",
    "    \n",
    "    \n",
    "def get_most_repeated_words_from_plot_summary_counter(plot_summary_counter):\n",
    "    plot_summary_dict = dict(plot_summary_counter)\n",
    "    \n",
    "    most_repeated_words = [word for word, count in plot_summary_dict.items() if count >= 15]\n",
    "    \n",
    "    return most_repeated_words\n",
    "\n",
    "\n",
    "\n",
    "def get_bag_of_words_representation_of_all_plots(plot_summary_tokens_series):\n",
    "    plot_summary_tokens_array = np.array(plot_summary_tokens_series)\n",
    "    plot_summary_counter = get_summary_as_dictionary_of_words(plot_summary_tokens_array[0])\n",
    "    \n",
    "    length_of_plot_summary_array = len(plot_summary_tokens_array)\n",
    "    \n",
    "    for summary_index in range(1, length_of_plot_summary_array):\n",
    "        plot_summary_counter = plot_summary_counter + get_summary_as_dictionary_of_words(plot_summary_tokens_array[summary_index].split())\n",
    "        \n",
    "    most_repeated_words = get_most_repeated_words_from_plot_summary_counter(plot_summary_counter)\n",
    "    return most_repeated_words\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def remove_non_most_repeated_words_from_summary(most_repeated_words, plot_summary_tokens_series):\n",
    "    \n",
    "    plot_summary_tokens_array = np.array(plot_summary_tokens_series)\n",
    "    length_of_plot_summary_array = len(plot_summary_tokens_array)\n",
    "    count = 0\n",
    "   \n",
    "    for summary_index in range(0, length_of_plot_summary_array):\n",
    "       \n",
    "        indices_of_words_to_be_removed = []\n",
    "        plot_summary_words_list = plot_summary_tokens_array[summary_index].split()\n",
    "        no_of_words_in_summary = len(plot_summary_words_list)\n",
    "        plot_summary_words_list = [word for word in plot_summary_words_list if word in most_repeated_words]\n",
    "        \n",
    "        plot_summary_tokens_array[summary_index] = ' '.join(plot_summary_words_list)\n",
    "    return plot_summary_tokens_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "most_repeated_words = get_bag_of_words_representation_of_all_plots(train_data)\n",
    "plot_summary = remove_non_most_repeated_words_from_summary(most_repeated_words, train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "def tfidf_vectorizer_on_train_and_test_data(plot_summary):\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    plot_summary_vectors = tfidf_vectorizer.fit_transform(plot_summary)\n",
    "    \n",
    "    return plot_summary_vectors\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_summary_vectors = tfidf_vectorizer_on_train_and_test_data(plot_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "def get_top_8000_features_from_plot_summary_vector(plot_summary_vectors, test_data):\n",
    "    plot_summary_vectors_new = SelectKBest(chi2, k = 8000).fit_transform(plot_summary_vectors, test_data)\n",
    "    return plot_summary_vectors_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_summary_vectors_new = get_top_8000_features_from_plot_summary_vector(plot_summary_vectors, test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42204"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(plot_summary_vectors_new.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def standardize_the_data_X(train_X):\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_X)\n",
    "    return scaler.transform(train_X)\n",
    "\n",
    "\n",
    "def normalize_the_data(train_data):\n",
    "    \n",
    "    train_data_normalized = normalize(train_data, norm = 'l1', axis = 0)\n",
    "    return train_data_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_summary_vectors_standardized = standardize_the_data_X(plot_summary_vectors.toarray())\n",
    "plot_summary_vectors_standardized = standardize_the_data_X(plot_summary_vectors_new.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# X_Y_data = get_train_and_label_data(plot_summary_vectors_new.toarray(), test_data)\n",
    "X_Y_data = get_train_and_label_data(plot_summary_vectors_standardized, test_data)\n",
    "train_X, train_Y = X_Y_data[0]\n",
    "test_X, test_Y = X_Y_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_X_array = train_X.toarray()\n",
    "train_Y_array = train_Y.values\n",
    "#test_X_array = test_X.toarray()\n",
    "test_Y_array = test_Y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_summary_vector_df = pd.DataFrame(plot_summary_vectors_new.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_Y_data = get_train_and_label_data(plot_summary_vector_df.values, test_data)\n",
    "train_X_ann, train_Y_ann = X_Y_data[0]\n",
    "test_X_ann, test_Y_ann = X_Y_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_weights_for_neural_networks(no_of_columns_in_train_x, hidden_neurons, no_of_diff_labels_in_train_Y):\n",
    "    weights_1 = np.random.random((no_of_columns_in_train_x, hidden_neurons[0]))\n",
    "    weights_2 = np.random.random((hidden_neurons[0], hidden_neurons[1])) * np.sqrt(2 / \n",
    "                                                                                    no_of_columns_in_train_x)\n",
    "    weights_3 = np.random.random((hidden_neurons[1], no_of_diff_labels_in_train_Y)) * np.sqrt(2 \n",
    "                                                                                        / hidden_neurons[0])\n",
    "    \n",
    "    return weights_1, weights_2, weights_3\n",
    "\n",
    "\n",
    "def get_biases_for_neural_networks(no_of_rows, hidden_neurons, no_of_diff_labels_in_train_Y):\n",
    "        \n",
    "    bias_1 = np.full((no_of_rows, hidden_neurons[0]), 0.1)\n",
    "    bias_2 = np.full((no_of_rows, hidden_neurons[1]), 0.1)\n",
    "    bias_3 = np.full((no_of_rows, no_of_diff_labels_in_train_Y), 0.1)\n",
    "    \n",
    "    return bias_1, bias_2, bias_3\n",
    "\n",
    "\n",
    "\n",
    "def ReLu(x):\n",
    "    indices = np.where(x <= 0)\n",
    "    x[indices] = 0\n",
    "    return x\n",
    "    \n",
    "def der_ReLu(x):\n",
    "    return 1. * (x > 0)\n",
    "\n",
    "\n",
    "def stable_softmax(x):\n",
    "    B = np.exp(x - np.max(x))\n",
    "    C = np.sum(B)\n",
    "    return B / C\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "class Neural_Network:\n",
    "    def __init__(self, train_X, train_Y, hidden_neurons, params):\n",
    "        self.training_data   = train_X\n",
    "        self.training_labels = train_Y\n",
    "        self.hidden_neurons  = hidden_neurons\n",
    "        \n",
    "\n",
    "        self.weights_1 = params[0]\n",
    "        self.weights_2 = params[1]\n",
    "        self.weights_3 = params[2]\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.bias_1 = params[3]\n",
    "        self.bias_2 = params[4]\n",
    "        self.bias_3 = params[5]\n",
    "        \n",
    "\n",
    "    def feed_forward_neural_network(self):\n",
    "        \n",
    "        self.layer_0 = self.training_data\n",
    "        \n",
    "        self.z_1 = np.dot(self.layer_0, self.weights_1) + self.bias_1\n",
    "        self.layer_1 = ReLu(self.z_1)\n",
    "        \n",
    "        self.z_2 = np.dot(self.layer_1, self.weights_2) + self.bias_2\n",
    "        self.layer_2 = ReLu(self.z_2)\n",
    "        \n",
    "        self.z_3 = np.dot(self.layer_2, self.weights_3) + self.bias_3\n",
    "        self.output = stable_softmax(self.z_3)\n",
    "     \n",
    "    \n",
    "\n",
    "    def backpropagate_neural_network(self):\n",
    "        \n",
    "        '''applying chain rule to find the derivative of loss function with respect to the \n",
    "        weights w_1, w_2, w_3'''\n",
    "        \n",
    "        self.diff_btw_output_labels = self.output - self.training_labels\n",
    "        prod_diff_w3_ReLu_z2 = np.dot(self.diff_btw_output_labels, self.weights_3.T) * der_ReLu(self.z_2)\n",
    "        temp_res_for_der_w1  = np.dot(prod_diff_w3_ReLu_z2, self.weights_2.T) * der_ReLu(self.z_1)\n",
    "        \n",
    "        \n",
    "        self.der_weights_3   = np.dot(self.layer_2.T, self.diff_btw_output_labels)\n",
    "        self.der_weights_2   = np.dot(self.layer_1.T, prod_diff_w3_ReLu_z2)\n",
    "        self.der_weights_1   = np.dot(self.layer_0.T, temp_res_for_der_w1)\n",
    "        \n",
    "        self.der_bias_3      = self.diff_btw_output_labels\n",
    "        self.der_bias_2      = prod_diff_w3_ReLu_z2\n",
    "        self.der_bias_1      = temp_res_for_der_w1\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3500\n",
    "\n",
    "def train(train_x, train_y, no_of_epochs, hidden_neurons, learning_rate, batch_size):\n",
    "    no_of_rows = len(train_x)\n",
    "    \n",
    "    no_of_rows_in_a_batch        = batch_size\n",
    "    no_of_columns_in_train_x     = np.shape(train_x)[1]\n",
    "    no_of_diff_labels_in_train_y = np.shape(train_y)[1]\n",
    "    \n",
    "    w1, w2, w3 = get_weights_for_neural_networks(no_of_columns_in_train_x, hidden_neurons, no_of_diff_labels_in_train_y)\n",
    "    b1, b2, b3 = get_biases_for_neural_networks(no_of_rows_in_a_batch, hidden_neurons, no_of_diff_labels_in_train_y)\n",
    "    params = w1, w2, w3, b1, b2, b3\n",
    "    cost = 0\n",
    "    \n",
    "    for epoch in range(0, no_of_epochs):\n",
    "        \n",
    "        for row in range(0, no_of_rows, batch_size):\n",
    "            batch_x = train_x[row : row + batch_size]\n",
    "            batch_y = train_y[row : row + batch_size]\n",
    "            \n",
    "            nn = Neural_Network(batch_x, batch_y, hidden_neurons, params)\n",
    "            nn.feed_forward_neural_network()\n",
    "            nn.backpropagate_neural_network()\n",
    "            \n",
    "            w1 += learning_rate * nn.der_weights_1\n",
    "            w2 += learning_rate * nn.der_weights_2\n",
    "            w3 += learning_rate * nn.der_weights_3\n",
    "            \n",
    "            b1 += learning_rate * nn.der_bias_1\n",
    "            b2 += learning_rate * nn.der_bias_2\n",
    "            b3 += learning_rate * nn.der_bias_3\n",
    "            \n",
    "            params = [w1, w2, w3, b1, b2, b3]\n",
    "            cost   = np.sum(nn.diff_btw_output_labels ** 2) / (2 * batch_size)\n",
    "            \n",
    "        print('epoch is - ' + str(epoch) + ' loss is - ' + str(cost))\n",
    "    return params, cost, nn\n",
    "            \n",
    "            \n",
    "parameters, cost, nn = train(train_X, train_Y_array[:, 0:1], 3, [200, 100], 0.02, batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
