{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob \n",
    "import csv\n",
    "import ast\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#####################################################################  ######\n",
    "\n",
    "'''read input csv file which has the information pertaining to the movie id, \n",
    "    name and genre to which a movie belongs to'''\n",
    "def read_Movie_Id_Name_Genre_File(filePath):\n",
    "    movie_id_name_genre_dicts_list = []\n",
    "    count = 0\n",
    "    with open(filePath) as file:\n",
    "        \n",
    "        movieFile = csv.reader(file, delimiter = '\\t')\n",
    "        for row in movieFile:\n",
    "            \n",
    "            movie_Id = row[0]\n",
    "            movie_Name = row[2]\n",
    "            genreListOfTheMovie = getTheGenresFromString(row[-1])\n",
    "            movie_id_name_genre_dicts_list.append(get_the_genres_dict_of_a_movie(movie_Id, \n",
    "                                                   movie_Name, genreListOfTheMovie))\n",
    "            \n",
    "    return pd.DataFrame(movie_id_name_genre_dicts_list)\n",
    "\n",
    "\n",
    "####################\n",
    "\n",
    "def getTheGenresFromString(genreString):\n",
    "    '''This function receives a genre string (of dictionary form) and \n",
    "         returns a list of genres to which a movie belongs to'''\n",
    "    genreOfTheMovie = []\n",
    "    \n",
    "    '''ast.literal_eval() converts a  string of type \"/m/09c7w0\": \"United States of America\" to a \n",
    "    dictionary of  {/m/09c7w0\": \"United States of America}'''\n",
    "    genreDictionary = ast.literal_eval(genreString)\n",
    "    for _, genre in genreDictionary.items():\n",
    "        genreOfTheMovie.append(genre)\n",
    "    \n",
    "    return genreOfTheMovie  \n",
    "\n",
    "\n",
    "#####################\n",
    "\n",
    "def get_the_genres_dict_of_a_movie(movie_Id, movie_Name, movie_genres_list):\n",
    "    '''input data has genres of type Action/Comedy and also Romantic comedy'''\n",
    "    whitespace = \" \"\n",
    "    backslash = \"/\"\n",
    "    movie_id_name_genre_dict = {'movie_Id' : movie_Id, 'movie_Name' : movie_Name}\n",
    "    genres = []\n",
    "    \n",
    "    for every_genre in movie_genres_list:\n",
    "        if whitespace in every_genre:\n",
    "            genres = genres + every_genre.split(whitespace)\n",
    "        elif backslash in every_genre:\n",
    "            genres = genres + every_genre.split(backslash)\n",
    "        else:\n",
    "            genres.append(every_genre)\n",
    "        \n",
    "    genres_list = create_genres_dict(genres)\n",
    "        \n",
    "    return add_genre_tuples_to_the_dict(movie_id_name_genre_dict, genres_list)\n",
    "\n",
    "\n",
    "\n",
    "#####################\n",
    "\n",
    "def create_genres_dict(genres):\n",
    "    \n",
    "    primary_genres = ['Action', 'Adventure', 'Animation', 'Biopic', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Family', 'Fantasy', 'Film-noir',\n",
    "         'History', 'Horror', 'Music', 'Musical', 'Mystery', 'News', 'Prison', 'Reality', 'Romance', 'Science Fiction', 'Sport', 'Thriller', 'War', 'Western']\n",
    "    \n",
    "    genres_list = []\n",
    "    if len(genres) == 1 and genres[0] in primary_genres:\n",
    "        #movie_id_name_genre_dict[genres[0]] = 1\n",
    "        genres_list.append(genres[0])\n",
    "    else:\n",
    "        for genre in genres:\n",
    "            if genre in primary_genres:\n",
    "                #movie_id_name_genre_dict[every_genre] = 1\n",
    "                genres_list.append(genre)\n",
    "                \n",
    "    return genres_list\n",
    "\n",
    "\n",
    "######################\n",
    "    \n",
    "def add_genre_tuples_to_the_dict(movie_id_name_genre_dict, genres_list):\n",
    "    for genre in genres_list:\n",
    "        movie_id_name_genre_dict[genre] = 1 \n",
    "    \n",
    "    return movie_id_name_genre_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "def read_movie_plot_summary_file(filePath):\n",
    "    movie_id_plot_summary_dicts_list = []\n",
    "    \n",
    "    \n",
    "    with open(filePath) as file:\n",
    "        moviePlotSummaryFile = csv.reader(file, delimiter = '\\t')\n",
    "        \n",
    "        for row in moviePlotSummaryFile:\n",
    "            #print(\"before - \" + str(len(row[1])))\n",
    "            movie_id_plot_summary_dict = {}\n",
    "            lemmatizedSummary = list(set(lemmatizeAndRemoveStopWordsFromTheSummary(row[1])))\n",
    "            \n",
    "            \n",
    "            '''converting a list of string words to a string separated by a whitespace'''\n",
    "            lemmatizedSummary_words_string = \" \".join(lemmatizedSummary)\n",
    "            \n",
    "            movie_id_plot_summary_dict['movie_Id'] = row[0]\n",
    "            movie_id_plot_summary_dict['plot_summary_tokens'] = lemmatizedSummary_words_string\n",
    "            \n",
    "            movie_id_plot_summary_dicts_list.append(movie_id_plot_summary_dict)\n",
    "                                      \n",
    "    return pd.DataFrame(movie_id_plot_summary_dicts_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####################\n",
    "\n",
    "def lemmatizeAndRemoveStopWordsFromTheSummary(plotSummary):\n",
    "    wordLemmatizer = WordNetLemmatizer()\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "#     punctuations = \"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\n",
    "    \n",
    "    summary_words = nltk.word_tokenize(plotSummary.lower())\n",
    "    modified_summary = []\n",
    "    #wordIndex = 0\n",
    "    \n",
    "    for word in summary_words:\n",
    "            \n",
    "        if not word in stopWords and len(word) > 2:\n",
    "            \n",
    "            lemmatizedWord = wordLemmatizer.lemmatize(remove_punctuation_marks(word), pos = \"v\")\n",
    "            modified_summary.append(lemmatizedWord)\n",
    "                \n",
    "    return modified_summary\n",
    "\n",
    "\n",
    "#######################\n",
    "def remove_punctuation_marks(word):\n",
    "    #return re.sub('[!\\\"#$%&\\'\\'()*+,-./:;<=>?@\\[\\]^_{|}~`\\s+]', '', word)\n",
    "    return re.sub('[0-9]+$', '', (re.sub('[!\\\"#$%&\\'\\'()*+,-./:;<=>?@\\[\\]^_{|}~`\\s+]', '', word)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###################################################################\n",
    "###################################################################\n",
    "\n",
    "def merge_the_movieIdNameGenre_plot_Dfs(movie_Id_Name_Genre_Df, movie_plot_Df):\n",
    "    return pd.merge(movie_Id_Name_Genre_Df, movie_plot_Df, on = 'movie_Id')\n",
    "\n",
    "\n",
    "####################################################################\n",
    "\n",
    "def fill_NaNs_with_Zeros(movie_id_name_genre_plot_df):\n",
    "    return movie_id_name_genre_plot_df.fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "###################################################################\n",
    "def get_transformed_train_test_data(movie_id_name_genre_plot_df):\n",
    "    \n",
    "    #test_columns = [column for column in movie_id_name_genre_plot_df.columns if column not in ['movie_Id', 'movie_Name', 'plot_summary_tokens']]\n",
    "    test_columns = [column for column in movie_id_name_genre_plot_df.columns if column not in ['plot_summary_tokens',\n",
    "                                                                                    'movie_Id', 'movie_Name']]    \n",
    "    train_data = movie_id_name_genre_plot_df.loc[:,'plot_summary_tokens']\n",
    "    test_data = movie_id_name_genre_plot_df.loc[:, test_columns]\n",
    "    return [train_data, test_data]\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "def get_train_and_label_data(train_data, test_data):\n",
    "    ## train_data is a list object\n",
    "    \n",
    "    train_X = train_data[0 : 35000]\n",
    "    train_Y = test_data.loc[0:34999,]\n",
    "\n",
    "    test_X = train_data[35000 : ]\n",
    "    test_Y = test_data.loc[35000: ,]\n",
    "    \n",
    "    return [[train_X, train_Y], [test_X, test_Y]]\n",
    "\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "def remove_rows_in_df_with_0_genre_tags(movie_id_name_genre_df):\n",
    "    indices_with_0_genre_tags = (np.where ((movie_id_name_genre_plot_df_zeros).iloc[:, 0 : 21].sum(axis = 1) == 0))[0]\n",
    "    return movie_id_name_genre_plot_df_zeros.drop(indices_with_0_genre_tags, axis = 0, inplace = True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_id_name_genre_df = read_Movie_Id_Name_Genre_File('/Users/vijay/Downloads/Datasets/MovieSummaries/movie.metadata.tsv')\n",
    "movie_id_plot_summary_df = read_movie_plot_summary_file('/Users/vijay/Downloads/Datasets/MovieSummaries/plot_summaries.txt')\n",
    "movie_id_name_genre_plot_df = merge_the_movieIdNameGenre_plot_Dfs(movie_id_name_genre_df, movie_id_plot_summary_df)\n",
    "\n",
    "movie_id_name_genre_plot_df_zeros = fill_NaNs_with_Zeros(movie_id_name_genre_plot_df)\n",
    "movie_id_name_genre_plot_df_zeros = remove_rows_in_df_with_0_genre_tags(movie_id_name_genre_plot_df_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_data = get_transformed_train_test_data(movie_id_name_genre_plot_df_zeros)\n",
    "train_data = train_test_data[0]\n",
    "test_data = train_test_data[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINDING AND UPDATING PLOT WITH MOST REPEATED WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "\n",
    "'''\n",
    "Out of all the words in plot_summary_tokens, only those which have occurred more than 15\n",
    "times in all the training samples were used in BoW representation. \n",
    "'''\n",
    "\n",
    "def get_summary_as_dictionary_of_words(summary):\n",
    "    return Counter(summary)\n",
    "    \n",
    "    \n",
    "def get_most_repeated_words_from_plot_summary_counter(plot_summary_counter):\n",
    "    plot_summary_dict = dict(plot_summary_counter)\n",
    "    \n",
    "    most_repeated_words = [word for word, count in plot_summary_dict.items() if count >= 15]\n",
    "    \n",
    "    return most_repeated_words\n",
    "\n",
    "\n",
    "\n",
    "def get_bag_of_words_representation_of_all_plots(plot_summary_tokens_series):\n",
    "    plot_summary_tokens_array = np.array(plot_summary_tokens_series)\n",
    "    plot_summary_counter = get_summary_as_dictionary_of_words(plot_summary_tokens_array[0])\n",
    "    \n",
    "    length_of_plot_summary_array = len(plot_summary_tokens_array)\n",
    "    \n",
    "    for summary_index in range(1, length_of_plot_summary_array):\n",
    "        plot_summary_counter = plot_summary_counter + get_summary_as_dictionary_of_words(plot_summary_tokens_array[summary_index].split())\n",
    "        \n",
    "    most_repeated_words = get_most_repeated_words_from_plot_summary_counter(plot_summary_counter)\n",
    "    return most_repeated_words\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def remove_non_most_repeated_words_from_summary(most_repeated_words, plot_summary_tokens_series):\n",
    "    \n",
    "    plot_summary_tokens_array = np.array(plot_summary_tokens_series)\n",
    "    length_of_plot_summary_array = len(plot_summary_tokens_array)\n",
    "    count = 0\n",
    "   \n",
    "    for summary_index in range(0, length_of_plot_summary_array):\n",
    "       \n",
    "        indices_of_words_to_be_removed = []\n",
    "        plot_summary_words_list = plot_summary_tokens_array[summary_index].split()\n",
    "        no_of_words_in_summary = len(plot_summary_words_list)\n",
    "        plot_summary_words_list = [word for word in plot_summary_words_list if word in most_repeated_words]\n",
    "        \n",
    "        plot_summary_tokens_array[summary_index] = ' '.join(plot_summary_words_list)\n",
    "    return plot_summary_tokens_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_repeated_words = get_bag_of_words_representation_of_all_plots(train_data)\n",
    "plot_summary = remove_non_most_repeated_words_from_summary(most_repeated_words, train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "def tfidf_vectorizer_on_train_and_test_data(plot_summary):\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df = 0.8, max_features = 10000)\n",
    "    plot_summary_vectors = tfidf_vectorizer.fit_transform(plot_summary)\n",
    "    \n",
    "    return tfidf_vectorizer, plot_summary_vectors\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer, plot_summary_vectors = tfidf_vectorizer_on_train_and_test_data(plot_summary)\n",
    "\n",
    "\n",
    "X_Y_data = get_train_and_label_data(plot_summary_vectors.toarray(), test_data)\n",
    "\n",
    "train_X, train_Y = X_Y_data[0]\n",
    "test_X, test_Y = X_Y_data[1]\n",
    "train_Y_array = train_Y.values\n",
    "test_Y_array = test_Y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OneVsRest + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regressor = LogisticRegression()\n",
    "OvR_classifier     = OneVsRestClassifier(logistic_regressor)\n",
    "\n",
    "OvR_classifier.fit(train_X, train_Y_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.3\n",
    "y_pred_probabilites = OvR_classifier.predict_proba(test_X)\n",
    "y_pred_new = (y_pred_probabilites >= threshold).astype(int)\n",
    "f1_score(test_Y_array, y_pred_new, average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OneVsRest + LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "linear_svc = LinearSVC(random_state=0, tol=1e-5)\n",
    "OvR_classifier_svc = OneVsRestClassifier(linear_svc)\n",
    "OvR_classifier_svc.fit(train_X, train_Y_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_svc = OvR_classifier_svc.predict(test_X)\n",
    "f1_score(test_Y_array, y_predicted_svc, average = 'micro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
